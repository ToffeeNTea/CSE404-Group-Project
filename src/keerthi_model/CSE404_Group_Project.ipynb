{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the Jupyter Notebook file for our CSE 404 Group Project. This will be run on Google Colab, and stored on Github at https://github.com/ToffeeNTea/CSE404-Group-Project.git\n",
        "\n",
        "IN ORDER TO SAVE YOUR CHANGES:\n",
        "1. Click ```File``` â†’ ```Save a copy in Github```\n",
        "  - Or just press CTRL + S, it brings up the prompt\n",
        "2. Select the repository ```ToffeeNTea/CSE404-Group-Project``` under branch ```main```\n",
        "3. Do not change the filepath\n",
        "4. Change the commit message\n",
        "5. Press ```OK```"
      ],
      "metadata": {
        "id": "Y38nhRDjI5JN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to load in the GitHub repository. Rerunning the cell will pull any changes to the repository from online. The working directory will be set to the repository.\n",
        "\n",
        "**NOTE: This does NOT update the current Colab notebook! You will need to open the file from GitHub again!**"
      ],
      "metadata": {
        "id": "wBDkInhiWTNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Opens the GitHub repository into the Google Colab environment\n",
        "# Pulls updates to files (except for the notebook itself) from the repo\n",
        "# If your notebook seems broken, press Runtime --> Disconnect and delete runtime\n",
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "if os.path.exists('/content/CSE404-Group-Project'):\n",
        "  !rm -rf /content/CSE404-Group-Project  # Delete old files\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/ToffeeNTea/CSE404-Group-Project.git\n",
        "# Working directory set to the repo\n",
        "%cd /content/CSE404-Group-Project\n",
        "\n",
        "# Utility stuff\n",
        "# Set these in Colab's \"Secrets\" (left side, key symbol)\n",
        "# try:\n",
        "#   GITHUB_EMAIL = userdata.get('GITHUB_EMAIL')\n",
        "#   GITHUB_USERNAME = userdata.get('GITHUB_USERNAME')\n",
        "#   GITHUB_TOKEN = userdata.get(\"GITHUB_TOKEN\")\n",
        "#   REPO_NAME = \"CSE404-Group-Project\"\n",
        "\n",
        "#   !git config --global user.email \"{GITHUB_EMAIL}\"\n",
        "#   !git config --global user.name \"{GITHUB_USERNAME}\"\n",
        "#   !git remote set-url origin https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\n",
        "# except:\n",
        "#   print(\"ERROR: Please set the secret environment variables\")\n",
        "#   print(\"\"\"You will need to create a Personal Access Token on GitHub.\n",
        "#     While logged in, go to https://github.com/settings/tokens\n",
        "#     Generate a new token (classic)\n",
        "#     Enable the following permissions:\n",
        "#       - repo\n",
        "#     Then copy paste into the Secrets (left side, key symbol)\n",
        "#     \"\"\")\n",
        "\n",
        "# def push_to_github(commit_message=\"Updated notebook from Colab\"):\n",
        "#   os.system(\"git add .\")\n",
        "#   os.system(f'git commit -m \"{commit_message}\"')\n",
        "#   os.system(\"git push origin main\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DncIpAgrK88N",
        "outputId": "8f26bb97-d150-4247-ade5-5db5b0c7cefc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'CSE404-Group-Project'...\n",
            "remote: Enumerating objects: 9585, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 9585 (delta 10), reused 7 (delta 2), pack-reused 9556 (from 4)\u001b[K\n",
            "Receiving objects: 100% (9585/9585), 641.26 MiB | 31.76 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "Updating files: 100% (10007/10007), done.\n",
            "/content/CSE404-Group-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to push updates from the local repository (Colab) to remote.\n",
        "\n",
        "**NOTE: This does NOT update the .ipynb file on GitHub! You will still need to save it manually using the steps above.**"
      ],
      "metadata": {
        "id": "GCuIh8iQjrDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_folder = \"../CSE404-Group-Project/database/dataset\"\n",
        "coords_folder = \"../CSE404-Group-Project/database/csv_data/coords.csv\"\n",
        "print(len(os.listdir(image_folder)))"
      ],
      "metadata": {
        "id": "5ZdD10wXXQrt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450b119b-52b9-436f-d778-a5ba7b4f36bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "image_folder = \"../CSE404-Group-Project/database/dataset\"\n",
        "label_file = \"../CSE404-Group-Project/database/csv_data/coords.csv\"\n",
        "\n",
        "# Load labels from CSV\n",
        "coords_df = pd.read_csv(label_file, header=None, names=[\"x_coord\", \"y_coord\"])\n",
        "# labels_df = pd.read_csv(label_file, header=None, names=[\"x_coord\", \"y_coord\"])\n",
        "labels = coords_df.values  # Convert to numpy array\n",
        "\n",
        "# Load and process images\n",
        "image_data = []\n",
        "for i in range(len(labels)):  # Assumes images are named 0.png, 1.png, ...\n",
        "    img_path = os.path.join(image_folder, f\"{i}.png\")\n",
        "\n",
        "    # Load and resize the image\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img_resized = cv2.resize(img, (128, 128))  # Resize to a uniform size\n",
        "    img_flattened = img_resized.flatten()  # Flatten to 1D array\n",
        "\n",
        "    image_data.append(img_flattened)\n",
        "\n",
        "# Convert to numpy array\n",
        "X = np.array(image_data)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "FovTtkBfzWpO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import hog\n",
        "\n",
        "def extract_hog_features(img):\n",
        "    resized_img = cv2.resize(img, (128, 128))  # Resize to 128x128\n",
        "    features, _ = hog(resized_img, orientations=9, pixels_per_cell=(8, 8),\n",
        "                      cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)\n",
        "    return features"
      ],
      "metadata": {
        "id": "1uGdpnFMzh9q"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_data = []\n",
        "for i in range(len(labels)):\n",
        "    img_path = os.path.join(image_folder, f\"{i}.png\")\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Extract HOG features\n",
        "    features = extract_hog_features(img)\n",
        "    feature_data.append(features)\n",
        "\n",
        "X = np.array(feature_data)"
      ],
      "metadata": {
        "id": "DgNGahnszh3D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Pb6Yv3yxzhro"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define grid size (e.g., 10x10 or any other size)\n",
        "grid_size = 10  # You can adjust the grid size for better granularity\n",
        "x_min, x_max = coords_df['x_coord'].min(), coords_df['x_coord'].max()\n",
        "y_min, y_max = coords_df['y_coord'].min(), coords_df['y_coord'].max()\n",
        "\n",
        "# Map coordinates to grid cells\n",
        "def map_to_grid(x, y, grid_size, x_min, x_max, y_min, y_max):\n",
        "    x_grid = int((x - x_min) / (x_max - x_min) * grid_size)\n",
        "    y_grid = int((y - y_min) / (y_max - y_min) * grid_size)\n",
        "    return x_grid * grid_size + y_grid  # Unique grid ID\n",
        "\n",
        "# Create grid labels for classification\n",
        "coords_df['grid_label'] = coords_df.apply(lambda row: map_to_grid(row['x_coord'], row['y_coord'],\n",
        "                                                                   grid_size, x_min, x_max, y_min, y_max), axis=1)"
      ],
      "metadata": {
        "id": "5pvCTdVq0Deh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X contains the flattened image or extracted HOG features\n",
        "# y_class contains the new grid-based labels\n",
        "X = np.array(feature_data)  # Feature array from images\n",
        "y_class = coords_df['grid_label'].values  # New classification labels"
      ],
      "metadata": {
        "id": "9u-FOMuW5wuJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression with a high iteration limit\n",
        "clf = LogisticRegression(max_iter=2000)  # Higher max_iter to ensure convergence\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict grid labels\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Qn-Yr5505woL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Classification report for precision, recall, and F1-score\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBjZR2sj5wXT",
        "outputId": "44930a34-7459-4d27-df3a-a854c7a3dd84"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[11  0  8 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 4  0 13 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  1  0]\n",
            " [ 0  0  1 ...  0  1  0]\n",
            " [ 0  0  0 ...  0  0  2]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.33      0.31      0.32        35\n",
            "           9       0.00      0.00      0.00         3\n",
            "          11       0.23      0.28      0.25        46\n",
            "          12       0.09      0.15      0.11        67\n",
            "          14       0.00      0.00      0.00        11\n",
            "          15       0.00      0.00      0.00        12\n",
            "          18       0.18      0.08      0.11        49\n",
            "          19       0.25      0.34      0.29        70\n",
            "          21       0.09      0.08      0.08        25\n",
            "          22       0.23      0.29      0.26       178\n",
            "          24       0.59      0.48      0.53        21\n",
            "          25       0.11      0.07      0.09        40\n",
            "          28       0.33      0.30      0.31        92\n",
            "          29       0.19      0.20      0.19        81\n",
            "          31       0.29      0.24      0.26        50\n",
            "          32       0.18      0.31      0.23       160\n",
            "          35       0.00      0.00      0.00         9\n",
            "          37       0.00      0.00      0.00         6\n",
            "          38       1.00      0.20      0.33        20\n",
            "          39       1.00      0.50      0.67         2\n",
            "          41       0.00      0.00      0.00        28\n",
            "          42       0.70      0.23      0.35        30\n",
            "          44       0.50      0.56      0.53        89\n",
            "          45       0.35      0.24      0.28        34\n",
            "          46       1.00      0.17      0.29         6\n",
            "          47       0.12      0.05      0.07        20\n",
            "          48       0.33      0.10      0.15        10\n",
            "          50       0.05      0.02      0.03        42\n",
            "          51       0.00      0.00      0.00        16\n",
            "          53       0.80      0.61      0.69        33\n",
            "          54       0.76      0.43      0.55        30\n",
            "          56       0.50      0.43      0.46        30\n",
            "          57       0.06      0.07      0.07        57\n",
            "          58       0.00      0.00      0.00         3\n",
            "          60       0.07      0.07      0.07        40\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         7\n",
            "          65       0.33      0.09      0.14        23\n",
            "          66       0.40      0.33      0.36         6\n",
            "          67       1.00      0.50      0.67         2\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         1\n",
            "          73       1.00      0.50      0.67         2\n",
            "          74       0.08      0.10      0.09        87\n",
            "          75       0.07      0.12      0.08        95\n",
            "          76       1.00      0.33      0.50         3\n",
            "          77       0.53      0.42      0.47        38\n",
            "          78       0.57      0.21      0.31        19\n",
            "          79       0.00      0.00      0.00         3\n",
            "          82       1.00      0.75      0.86         4\n",
            "          83       0.00      0.00      0.00         7\n",
            "          84       0.12      0.14      0.13        57\n",
            "          85       0.23      0.28      0.25        69\n",
            "          86       0.18      0.16      0.17        38\n",
            "          87       0.60      0.27      0.38        22\n",
            "          88       0.86      0.22      0.35        27\n",
            "          89       0.50      0.20      0.29         5\n",
            "          91       0.00      0.00      0.00         1\n",
            "          92       1.00      0.50      0.67         2\n",
            "          93       0.00      0.00      0.00         3\n",
            "          94       0.00      0.00      0.00        10\n",
            "          95       0.14      0.11      0.12         9\n",
            "          96       1.00      0.22      0.36         9\n",
            "\n",
            "    accuracy                           0.23      2000\n",
            "   macro avg       0.33      0.20      0.23      2000\n",
            "weighted avg       0.27      0.23      0.23      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Load ResNet and remove classification layer\n",
        "model = models.resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*list(model.children())[:-1])  # Remove final layer\n",
        "model.eval()\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Feature extraction function\n",
        "def extract_features(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "    with torch.no_grad():  # No gradients needed\n",
        "        features = model(image)\n",
        "    return features.squeeze().numpy().flatten()  # Flatten to 1D\n",
        "\n",
        "# Load coordinate data (CSV file)\n",
        "# coords_df = pd.read_csv(\"/mnt/data/coords.csv\")\n",
        "\n",
        "# Define image folder path\n",
        "# image_folder = \"../../database/dataset\"\n",
        "\n",
        "# Extract features and map labels\n",
        "features, labels = [], []\n",
        "\n",
        "# Define grid size for classification (adjust if needed)\n",
        "grid_size = 10\n",
        "x_min, x_max = coords_df['x_coord'].min(), coords_df['x_coord'].max()\n",
        "y_min, y_max = coords_df['y_coord'].min(), coords_df['y_coord'].max()\n",
        "\n",
        "# Map coordinates to grid cells\n",
        "def map_to_grid(x, y, grid_size, x_min, x_max, y_min, y_max):\n",
        "    x_grid = int((x - x_min) / (x_max - x_min) * grid_size)\n",
        "    y_grid = int((y - y_min) / (y_max - y_min) * grid_size)\n",
        "    return x_grid * grid_size + y_grid\n",
        "\n",
        "# Extract features and assign grid labels\n",
        "for idx, row in coords_df.iterrows():\n",
        "    image_path = os.path.join(image_folder, f\"{idx}.png\")\n",
        "    if os.path.exists(image_path):\n",
        "        feature_vector = extract_features(image_path)\n",
        "        features.append(feature_vector)\n",
        "        grid_label = map_to_grid(row['x_coord'], row['y_coord'], grid_size, x_min, x_max, y_min, y_max)\n",
        "        labels.append(grid_label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(features)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "PC_0TYxg9ul9",
        "outputId": "52c93963-c68d-4a1b-8328-752c3ed639ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:01<00:00, 89.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "clf = LogisticRegression(max_iter=2000)  # Higher max_iter for convergence\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "Ffja3kVgNNd_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "cPFZ_BgENY8f",
        "outputId": "f2cc8547-92a8-452a-b3e7-e81a7ac3e8a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[24  0  5 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 4  0 27 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  2  2  0]\n",
            " [ 0  0  0 ...  2  2  1]\n",
            " [ 0  0  1 ...  0  1  4]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.56      0.69      0.62        35\n",
            "           9       0.00      0.00      0.00         3\n",
            "          11       0.43      0.59      0.50        46\n",
            "          12       0.28      0.36      0.31        67\n",
            "          14       0.29      0.18      0.22        11\n",
            "          15       0.00      0.00      0.00        12\n",
            "          18       0.59      0.45      0.51        49\n",
            "          19       0.38      0.43      0.40        70\n",
            "          21       0.30      0.24      0.27        25\n",
            "          22       0.44      0.47      0.46       178\n",
            "          24       0.68      0.62      0.65        21\n",
            "          25       0.28      0.25      0.26        40\n",
            "          28       0.66      0.73      0.69        92\n",
            "          29       0.44      0.49      0.47        81\n",
            "          31       0.49      0.40      0.44        50\n",
            "          32       0.42      0.49      0.45       160\n",
            "          35       0.00      0.00      0.00         9\n",
            "          37       0.00      0.00      0.00         6\n",
            "          38       0.70      0.35      0.47        20\n",
            "          39       0.50      0.50      0.50         2\n",
            "          41       0.28      0.25      0.26        28\n",
            "          42       0.59      0.43      0.50        30\n",
            "          44       0.78      0.78      0.78        89\n",
            "          45       0.57      0.59      0.58        34\n",
            "          46       0.25      0.17      0.20         6\n",
            "          47       0.29      0.20      0.24        20\n",
            "          48       0.25      0.20      0.22        10\n",
            "          50       0.16      0.12      0.14        42\n",
            "          51       0.24      0.25      0.24        16\n",
            "          53       0.76      0.79      0.78        33\n",
            "          54       0.74      0.67      0.70        30\n",
            "          56       0.72      0.70      0.71        30\n",
            "          57       0.23      0.23      0.23        57\n",
            "          58       0.00      0.00      0.00         3\n",
            "          60       0.31      0.25      0.28        40\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         7\n",
            "          65       0.31      0.22      0.26        23\n",
            "          66       0.75      0.50      0.60         6\n",
            "          67       0.33      0.50      0.40         2\n",
            "          68       0.00      0.00      0.00         5\n",
            "          69       0.00      0.00      0.00         1\n",
            "          73       1.00      0.50      0.67         2\n",
            "          74       0.26      0.28      0.27        87\n",
            "          75       0.30      0.43      0.35        95\n",
            "          76       0.67      0.67      0.67         3\n",
            "          77       0.71      0.66      0.68        38\n",
            "          78       0.44      0.42      0.43        19\n",
            "          79       0.00      0.00      0.00         3\n",
            "          82       1.00      0.75      0.86         4\n",
            "          83       0.67      0.29      0.40         7\n",
            "          84       0.31      0.39      0.35        57\n",
            "          85       0.36      0.32      0.34        69\n",
            "          86       0.34      0.37      0.35        38\n",
            "          87       0.47      0.32      0.38        22\n",
            "          88       0.76      0.70      0.73        27\n",
            "          89       0.50      0.20      0.29         5\n",
            "          91       1.00      1.00      1.00         1\n",
            "          92       1.00      0.50      0.67         2\n",
            "          93       0.00      0.00      0.00         3\n",
            "          94       0.33      0.20      0.25        10\n",
            "          95       0.25      0.22      0.24         9\n",
            "          96       0.50      0.44      0.47         9\n",
            "\n",
            "    accuracy                           0.44      2000\n",
            "   macro avg       0.41      0.36      0.38      2000\n",
            "weighted avg       0.44      0.44      0.43      2000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}